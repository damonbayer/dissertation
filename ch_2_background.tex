\chapter{Background}
\label{ch:background}
% Aiming to hit around page 30 by the end
\section{Bayesian Inference}
\label{ch_2:sec:bayesian-mcmc}

When performing Bayesian statistical inference, we aim to quantify uncertainty about parameters of interest, \( \btheta \), conditional on some observed data, \( \bX \).
To accomplish this, we employ a prior distribution, which expresses our beliefs about \( \btheta \) without data, and a sampling distribution for \( \bX \).
Formally, by Bayes' theorem and the law of total probability, we have 

\begin{equation}
    \pi \left( \btheta \mid \bX \right) = \frac{\pi \left( \bX \mid \btheta \right) \pi \left( \btheta \right)}{\pi \left(  \bX \right)} = \frac{\pi \left( \bX \mid \btheta \right) \pi \left( \btheta \right)}{\int \pi \left( \bX \mid \btheta \right) \pi \left( \btheta \right) \rmd \btheta},
\end{equation}

where \( \pi \left( \bX \mid \btheta \right) \) is the sampling distribution for \( \bX \) and \( \pi \left( \btheta \right) \) is the prior distribution.
Because \( \pi \left( \bX \right) \) is constant, with respect to \( \btheta \), we typically work with the unnormalized posterior

\begin{equation}
    \pi \left( \btheta \mid \bX \right) \propto \pi \left( \bX \mid \btheta \right) \pi \left( \btheta \right).
\end{equation}

In practice, the posterior distribution, \( \pi \left( \btheta \mid \bX \right) \), rarely exists in closed form.
However, we can sample from the posterior distribution via Markov chain Monte Carlo, wherein we construct a Markov Chain whose stationary distribution is the posterior distribution.
Throughout this dissertation, we rely on the Hamiltonian Monte Carlo method to perform this sampling.

Briefly, Hamiltonian Monte Carlo is a form of the Metropolisâ€“Hastings algorithm.
In the Metropolis-Hastings algorithm, at each step in the Markov chain, a new state, \( \btheta^\prime \), is proposed by sampling from a proposal distribution \( q \).
The proposed value \( \btheta^\prime \), is accepted with probability 

\begin{align}
\alpha \left( \btheta, \btheta^\prime \right)   =&   \min \left\{ 1, \frac{\pi \left( \btheta^\prime \mid \bX \right)}{\pi \left( \btheta \mid \bX \right)} \frac{q \left( \btheta \mid \btheta^\prime \right)}{q \left( \btheta^\prime \mid \btheta  \right)} \right\}    \\
=&  \min \left\{ 1, \frac{\pi \left( \bX \mid \btheta^\prime  \right)}{\pi \left( \bX \mid \btheta \right)} \frac{\pi \left( \btheta^\prime \right)}{\pi \left( \btheta \right)} \frac{q \left( \btheta \mid \btheta^\prime \right)}{q \left( \btheta^\prime \mid \btheta  \right)} \right\}.
\end{align}

Efficiently sampling using Metropolis-Hastings requires selecting a proposal distribution that produces proposals that are far enough away from the current parameters to explore the parameter space quickly, but not so widely distributed that they have a very low probability of acceptance.
As \( \btheta \) increases in dimensionality, striking this balance becomes more difficult, and Markov chains constructed with relatively simple algorithms tend to become ``stuck" in a small region, unable to explore the full posterior.
Compared to simpler algorithms (e.g. random walk Metropolis-Hastings), Hamiltonian Monte Carlo can more efficiently generate proposals by using Hamiltonian dynamics, where we augment our parameter space with ``momentum" variables which guide the proposals through regions of high probability.
For a detailed overview, see \citet{betancourt2018conceptual} and \citet{neal2011mcmc}.

\section{Confidence Distributions}
\label{ch_2:sec:confidence_distributions}
Confidence distributions are a frequentist estimators of a parameter, although they are historically associated with the fiducial distribution from fiducial statistics \citep{Xie2013}.
They are ``distribution estimators," similar in spirit to the bootstrap and the posterior distribution from Bayesian inference (see section \ref{ch_2:sec:bayesian-mcmc}).
A modern definition and short history of confidence distributions is available in \citet{Xie2013}.
For clarity with respect to discrete data, which is the focus of our use of confidence distributions in Chapter~\ref{ch:content_1}, we present a more classical derivation, which relies on previously derived confidence interval procedures.

Under the classical derivation, we can create random variables associated with the lower and upper confidence distributions from two one-sided, nested, confidence interval procedures.
For a nested confidence interval procedure, the $(1-\alpha_1)$ interval is a subset of the $(1-\alpha_2)$ interval whenever $(1-\alpha_1) \leq (1-\alpha_2)$.
Denote those one-sided intervals by $[L({\bf x},1-\alpha), \infty)$ and $(-\infty, U({\bf x},1-\alpha)]$, which are defined for any observed data ${\bf x}$, and $(1-\alpha) \in (0,1)$.
For fixed ${\bf x}$, define the lower and upper confidence distribution random variables as $L({\bf x}, A)$ and $U({\bf x},B)$, where $A$ and $B$ are independent uniform random variables.
Then a $100(1-\alpha)\%$ central confidence interval has the $(\alpha/2)$th quantile of $L({\bf x}, A)$ as the lower limit, and the $(1-\alpha/2)$th quantile of $U({\bf x},B)$ as the upper limit.

As an example, let \( X_1, \ldots, X_n \) be independently and identically distributed \( \textrm{Normal}(\mu, \sigma^2) \).
With \( \sigma^2 \) known, a \( (1 - \alpha) \)\% confidence interval for \( \mu \) is
\begin{equation}
\left[ \bar{x} - z_{\alpha/2} \frac{\sigma}{\sqrt{n}}, \bar{x} + z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \right],
\label{ch_2:eqn:CI_normal_1}
\end{equation}
where \( \bar{x} \) is the sample mean and \( z_{\alpha / 2} \) satisfies \( P(Z \geq z_{\alpha / 2}) = \alpha / 2 \) and \( Z \sim \textrm{Normal}(0,1) \).

Letting \( L_\mu (\bx, q) = U_\mu (\bx, q) = \bar{x} - \frac{\sigma}{\sqrt{n}} \Phi^{-1}(q) \), we have \( L_\mu(\bx, A) \sim \textrm{Normal} \left( \bar{x}, \frac{\sigma^2}{n} \right) \) and \( U_\mu(\bx, A) \sim \textrm{Normal} \left( \bar{x}, \frac{\sigma^2}{n} \right) \)

We now note that \eqref{ch_2:eqn:CI_normal_1} is equivalent to
\begin{equation}
    \left[ F_{L_{\mu}}^{-1}(\alpha / 2), F_{U_{\mu}}^{-1}(1- \alpha / 2) \right],
\end{equation}
where \( F_Y^{-1} \) indicates the inverse of the cumulative distribution function of a random variable \( Y \).
Thus, we say \( L_\mu \) and \( U_\mu \) are the lower and upper confidence distribution random variables for \( \mu \).

\section{Mathematical Models for the Spread of Infectious Diseases}
\label{sec:math_models}

\subsection{Compartmental Models}
\label{ch_2:sec:compartmental_models}
The compartmental model is a popular tool used to describe the spread of infectious disease.
The most basic and prominent of these is the Susceptible-Infected-Removed (SIR) model, which we use as an example in this section.
In these models, the population of interest is divided into compartments which indicate a disease status (e.g., susceptible, infectious, and recovered), and individuals transition between compartments at a specified rate
(e.g. transitions from infectious compartment to recovered compartment happen at a rate proportional to the number of infectious individuals).
Overviews of mechanistic compartmental models for disease dynamics can be found in \citet{anderson1992infectious, Brauer2008, keeling2011modeling, 10.1093/aje/kww021}.

These models can be represented deterministically or stochastically.
In the stochastic treatment, the model is formulated as a continuous-time Markov chain.
For the SIR model, we write the state vector at the time \( t \) as \( \bX(t) = (S(t), I(t), R(t)) \).
Where \( S(t), I(t) \), and \( R(t) \) indicate the number of people from the population in the susceptible, infectious, and recovered compartments, respectively.
In this model, individuals may only transition from susceptible to infectious \( (S \to I) \) or from infectious to recovered \( (I \to R) \) (see Figure~\ref{ch_2:fig:SIR_diagram}).
\begin{figure}
    \centering
% https://q.uiver.app/?q=WzAsMyxbMCwwLCJTIl0sWzEsMCwiSSJdLFsyLDAsIlIiXSxbMCwxXSxbMSwyXV0=
\begin{tikzcd}
	S & I & R
	\arrow[from=1-1, to=1-2]
	\arrow[from=1-2, to=1-3]
\end{tikzcd}
    \caption[The basic SIR compartmental model.]{The basic SIR compartmental model.
    Only transitions from susceptible to infectious \( (S \to I) \) and from infectious to recovered \( (I \to R) \) are allowed.}
    \label{ch_2:fig:SIR_diagram}
\end{figure}
Infection events are assumed to happen at the rate \( \frac{\beta}{N} S(t) I(t) \), where \( \beta \) is the contact rate
Recovery events occur at a rate \( \gamma I \), where \( \gamma \) is the recovery rate.
If we constrain the population to be closed, as is typically done, we have \(  S(t) + I(t) + R(t) = N \), where \( N \) is the total population size.
Thus, the state can be represented by any two elements of \( \bX (t) \), with the third implicitly defined as \( N \) minus the sum of the other two.
We use the full representation for clarity.
The diagram in Figure~\ref{ch_2:fig:stocastic_SIR} represents the system's evolution from one state to the next.

\begin{figure}
    \centering
% https://q.uiver.app/?q=WzAsMyxbMCwxLCIoUyh0KSwgSSh0KSwgUih0KSkiXSxbMiwwLCIoUyh0KS0xLCBJKHQpKzEsIFIodCkpIl0sWzIsMiwiKFModCksIEkodCktMSwgUih0KSsxKSJdLFswLDEsIlxcZnJhY3tcXGJldGF9e059U0kiXSxbMCwyLCJcXGdhbW1hIEkiLDJdLFsxLDAsIlxcdGV4dHtJbmZlY3Rpb259IiwwLHsic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoibm9uZSJ9LCJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzIsMCwiXFx0ZXh0e1JlY292ZXJ5fSIsMix7InN0eWxlIjp7ImJvZHkiOnsibmFtZSI6Im5vbmUifSwiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dXQ==
\begin{tikzcd}
	&& {(S(t)-1, I(t)+1, R(t))} \\
	{(S(t), I(t), R(t))} \\
	&& {(S(t), I(t)-1, R(t)+1)}
	\arrow["{\frac{\beta}{N}SI}", from=2-1, to=1-3]
	\arrow["{\gamma I}"', from=2-1, to=3-3]
	\arrow["{\text{Infection}}", draw=none, from=1-3, to=2-1]
	\arrow["{\text{Recovery}}"', draw=none, from=3-3, to=2-1]
\end{tikzcd}
    \caption[State transition diagram for SIR model.]{State transition diagram for SIR model at time \( t \) with current state \( (S(t) \), \( I(t) \),  \( R(t)) \).
    The population experiences an infection and transitions to the state \( (S(t)-1, I(t)+1, R(t)) \) at the rate \( \frac{\beta}{N}SI \).
    The population experiences a recovery and transitions to the state \( (S(t), I(t)-11, R(t)+1) \) at the rate \( \gamma I \).
    }
    \label{ch_2:fig:stocastic_SIR}
\end{figure}

The deterministic treatment of a stochastic compartmental model is a system of ordinary differential equations, which is the infinite population limit of the stochastic model \citep{Greenwood2009}.
For the SIR model, we have 
\begin{equation}
    \deriv{S}{t} = -\beta \frac{SI}{N}, \qquad
    \deriv{I}{t} = \beta \frac{SI}{N} - \gamma I, \qquad
    \text{and} \qquad
    \deriv{R}{t} = \gamma I
    \text{.}
\label{ch_2:SIR_diff_eq}
\end{equation}

Compared to the deterministic models, the stochastic models are more computationally difficult to work with, but they are particularly useful when there are few infections, as at the beginning or end of an outbreak.
In these scenarios, the randomness involved at the subject level can have a major impact on the course of the outbreak.
In an extreme case, one infectious individual could be introduced to a population, but, by chance, not infect anyone else in the population.
Then, no outbreak occurs.
In the deterministic treatment, the outbreak is guaranteed to begin and will last indefinitely because \( S(t) \), \(  I(t) \), and \( R(t) \) will always be greater than 0, due to the behavior of the differential equations in \eqref{ch_2:SIR_diff_eq}.
The benefit to using the less realistic deterministic models is that they are more computationally feasible and tend to work well when all compartments are suitably large \citep{doi:10.1098/rspb.2015.0347}.
This work focuses on the deterministic models, as we typically work in large population settings, where an outbreak is ongoing.

The basic SIR model can be augmented with more compartments, as well as time-varying parameters, to more realistically represent an outbreak.
Additional compartments can account for different disease states (e.g. exposed, vaccinated, or deceased), competing disease variants, or stratification of the population (e.g. by age or location).
It is also possible to model a perpetual outbreak by allowing reinfection after recovery (e.g. \(R \to S\) transitions).
Time-varying parameters can also model changes in behavior or policy (e.g. stay at home orders and restaurant closures) that cannot be captured by the constant parameters.

Figure~\ref{ch_2:fig:SEIRDS_diagram} presents a model demonstrating some of these features, including additional compartments for individuals who are exposed, but not infections (\( E \)) and those who are deceased (\( D \)), as well as reinfections after recovery.

\begin{figure}
    \centering
% https://q.uiver.app/?q=WzAsNSxbMCwxLCJTIl0sWzQsMSwiSSJdLFs1LDAsIlIiXSxbMiwxLCJFIl0sWzUsMiwiRCJdLFsxLDJdLFswLDNdLFszLDFdLFsxLDRdLFsyLDAsIiIsMSx7ImN1cnZlIjo1fV1d
\begin{tikzcd}[column sep=scriptsize]
	&&&&& R \\
	S && E && I \\
	&&&&& D
	\arrow[from=2-5, to=1-6]
	\arrow[from=2-1, to=2-3]
	\arrow[from=2-3, to=2-5]
	\arrow[from=2-5, to=3-6]
	\arrow[curve={height=30pt}, from=1-6, to=2-1]
\end{tikzcd}
    \caption{Diagram of an augmented SIR model.
    The states in this model are susceptible \( (S) \), exposed \( (E) \), infectious \( (I) \), Recovered \( (R) \) and Deceased \( D \).}
    \label{ch_2:fig:SEIRDS_diagram}
\end{figure}

\subsection{Data Integration}
\label{ch_2:sec:data_integration}
Broadly, we can categorize data used for this modeling as either actively or passively collected surveillance data.
Common examples of passively collected incidence data include daily counts of diagnostic tests, confirmed cases, and deaths, as well as genetic sequences of viruses or bacteria that cause diseases.
We refer to this as incidence data because each count is assumed to correspond to a new event during that time.
In contrast, we work with prevalence data, which is reflective of the frequency of a condition at a given time.
Examples of prevalence data include counts of current hospital or intensive care unit (ICU) occupants with a certain disease or the number of seropositive tests from a carefully sampled population.
We also loosely differentiate between data that is passively or actively collected.
Passively collected data like case counts and hospital occupancy is reported routinely by health care agencies and is generally more affordable to collect because it doesn't require a formal study to be conducted.
Actively collected data is often the result of a survey that has been deliberately designed and conducted to measure some aspect of a population, like the current number of people infected with a disease or the contact rates between different subpopulations.
This data is more expensive to collect, but can provide information that is difficult to gather through passive surveillance.
% could mention mobility data somewhere here

As alluded to in Section~\ref{ch_2:sec:compartmental_models}, modelers can construct compartmental models which encode arbitrarily complex dynamics.
While simulating from these models can be straightforward, connecting them to real-world data to perform statistical inference is a non-trivial task.
Broadly, a model uses a compartmental model and basic parameters, like the reproductive number, duration of infectiousness, and infection-fatality ratio, to model the size of a compartment at a series of time points.
These compartment sizes can be mapped to prevalence data with some emission distribution or loss function.
To integrate incidence data, we can keep track of the cumulative incidence (e.g. by counting the cumulative number of \( S \to I \) transitions) at each time point and compute the difference between successive times.
Then these differences can be mapped to incidence data with an emission distribution or loss function.
For example, let \( N_{S I}(t_l) \) be the cumulative number of \( S \to I \) transitions by time \( t = l \), as reported by the compartmental model.
Then \( \Delta N_{S I}(t_l) = N_{S I}(t_l) - N_{S I}(t_{l-1}) \) is the number new \( S \to I \) transitions between time \( t = l \) and time \( t = l - 1 \).
We can link this to \( C_l \), the number of observed cases between time \( t = l \) and time \( t = l - 1 \) by
\begin{equation}
C_l \sim \text{Negative binomial}\left (\mu_l =  \rho \times \Delta N_{S I}(t_l),\ {\sigma^2_l} = \mu_l (1 + \mu_l / \phi )\right ),
\label{ch_2:eqn:example_case_emission}
\end{equation}
where \( \rho \) is a case reporting rate, reflecting that not all people who become infected with a disease are tested, and \( \phi \) is some over-dispersion parameter.

The specifics of the data and compartmental model used can make the mapping from the compartment sizes and differences challenging.
In some cases, the distinction between incidence and prevalence data is not clear.
For example, when someone receives a positive test result for an infectious disease with a relatively short period of infectiousness, they may receive several tests in the following weeks to approximate whether they are still infectious.
In that case, each positive test is not necessarily reflective of a new disease case.
The link between positive test results and the underlying cases may also be complicated by imperfect specificity and sensitivity of the tests.
Similarly, there is ambiguity about the disease's relevance to an event reported in the data.
Someone admitted to the hospital with a broken leg may also have an infectious disease and be reported as part of the hospital's passive surveillance data.
Thus, the hospitalization data may not be truly reflective of the number of people with severe disease cases.
Often, there is a significant delay between when data is captured and when it is reported.
When performing inference or predictions in real time, it may appear that there are fewer cases in one week than in the previous week, simply due to the most recent week's tests not yet being fully reported.
Furthermore, reporting practices for passive surveillance data may be inconsistent.
The inconsistency could be due to any number of reasons.
For example, many testing locations may not be open on the weekends or holidays, making it appear that new cases are less common on those days.
Inconsistencies may also be due to policy changes, changes in funding, shifting demographics among the infected population, or even accidental non-reporting.

While overcoming each of these individual complications is itself difficult and requires modelers to make many subtle choices, there is additional complexity involved in integrating each of the data streams in a unified model.
This additional complexity can be worth pursuing, as integrating more data sources may make more model parameters identifiable.
In \citet{DeAngelis2015four}, the authors present four important challenges in data integration when modeling infectious diseases.
The first is the weighting of evidence.
When combining data streams of varying quality, it is desirable to rely more on the high-quality data than the low-quality data.
This can be done simply by excluding the low-quality data altogether, or more complexly by formally modeling data quality issues.
Under the umbrella of model criticism, the authors present identifiability, conflict, and influence as areas of concern.
In Section~\ref{ch_1:sec:motivating_examples}, we provided an example of different data sources presenting conflicting evidence about underlying disease dynamics.
Understanding how each data stream informs modeling results and how their apparent differences are reconciled is a key challenge.
In addition, the authors cite issues related to computational complexity when combining many data sources, as well as potential problems of dependencies between the datasets included in the model.

\section{Forecasting Infectious Disease Outbreaks}
\label{sec:forecasting_techniques_and_assessment}

While compartmental models are a common framework for forecasting infectious disease outbreaks, other methods are available for the task.
A sample of these methods are gathered in the COVID-19 Forecast Hub, which collected and synthesized forecasts from a variety of modelers with diverse approaches throughout the COVID-19 pandemic \citep{Cramer2022Evaluation}.
Among the methods presented are those using compartmental models, along with approaches common to other areas of statistics, such as time series and machine learning methods.
In addition, \citet{Cramer2022Evaluation} evaluates an ensemble forecast, which combines the models into a single forecast, which is generally shown to have superior performance to any of the individual models.
These methods typically use the data described in Section~\ref{ch_2:sec:data_integration}, but some incorporate additional data sources, such as mobility data collected from mobile phones.

\subsection{Forecast Assessment}

Methods used for forecast assessment differ based on the format of the forecasts.
We begin with detailed exploration of forecasts in the form of full probability distributions, which we use throughout this work.
We end this section with a brief overview of other forecast evaluation methods.

Probabilistic forecasts are probability distributions over future events.
In this section, let \( F(x) \) denote the cumulative distribution function of a forecast.
We assume that new data are realized samples from some underlying probability distribution \( G \), which is only known to nature.
Thus, an ideal forecast is \( F = G \).
Because \( G \) is unknown, assessing whether \(F = G\) is impossible.
The sharpness principle conjectures that issuing a forecast that maximizes sharpness, subject to calibration, is equivalent to making an ideal forecast \citep{Gneiting2007Probabilistic}.

In plain terms, a forecast is well-calibrated if events that are forecasted to have a \( y \)\% chance of happening, are actually observed \( y \)\% of the time.
Being well-calibrated is insufficient for a forecast to be ``good."
For example, consider the ``climatological" forecaster, who issues forecasts based on the marginal distribution of the predictand.
In the context of infectious disease modeling, a climatological forecaster could issue a forecast for daily influenza cases by issuing the same forecast every day, where their forecast is simply a smoothed estimate of daily influenza cases from the last decade.
Assuming that influenza case counts in the future are, on average, distributed as they were in the last decade, this forecaster will be well-calibrated.
However, competing forecasters should be able to make narrower forecasts while maintaining calibration, perhaps by using data from the preceding week to predict case counts the next week.
These forecasts are clearly more useful and are said to be ``sharper."

In practice, calibration and sharpness can be assessed by both numerical and graphical summaries.
Calibration is often assessed by computing the probability integral transform (PIT), \( p = F(x) \) for each forecast, \( F \), and its corresponding observed value, \( x \), for all forecast times.
If the forecasts are well-calibrated, \( p_t \sim \operatorname{Uniform}(0,1) \).
Visual inspection of a histogram of PIT values for uniformity can, therefore, diagnose issues of miscalibration.
Calibration can also be assessed by computing the empirical coverage of prediction intervals.
Sharpness can be assessed by visually inspecting the predictive distributions or by computing summaries like interval widths or variance of the distribution.

To numerically assess both sharpness and calibration, a strictly proper scoring rule can also be used.
We denote a score for a probabilistic forecast \( F \) and observed outcome \( x \) by \( s(F, x) \).
Generally, the forecaster wishes to minimize the score, which can be considered a penalty or cost function.
A scoring rule is said to be proper when the expected value of the score is minimized when \( F = G \), where $x \sim G$.
The scoring rule is strictly proper when this minimum is unique.

For continuous variables, the continuous ranked probability score \eqref{ch_2:eqn:crps} is a popular scoring rule with attractive properties.
In contrast to other scoring rules, it is based on the cumulative distribution function of the predictive distribution, rather than the density function, which may not exist in some contexts.
Additionally, the CRPS is sensitive to distance, meaning that it gives credit to forecasts which assign high probability to values near \( x \), even if they do not assign high probability to \( x \) itself.
For a discussion of scoring rules, see \citet{gneiting2007strictly}.
\begin{equation}
    \operatorname{CRPS}(F, x) = \int_{-\infty}^{\infty} \left( F(y) - \mathds{1} \left( y \geq x \right) \right)^2 \rmd y
    \label{ch_2:eqn:crps}
\end{equation}
Predictive distributions arise naturally from the posterior predictive distribution in the Bayesian setting.
The posterior predictive distribution is the distribution of possible unobserved values, \( \bX^* \), conditional on the observed values, \( \bX \).
Following the notation from Section~\ref{ch_2:sec:bayesian-mcmc}, the posterior predictive distribution is \( \pi \left( \bX^* \mid \bX \right) = \int \pi \left( \bX^* \mid \btheta \right) \pi \left( \btheta \mid \bX \right) \rmd \btheta \).
When working in the context of MCMC (Section~\ref{ch_2:sec:bayesian-mcmc}), the posterior predictive distribution, \( F \), is not available in closed form.
Rather, we only have some finite number of samples from the posterior distribution, \( \left( \btheta_i, \ldots \btheta_n \right) \).
\citet{kruger2021predictive} presents three options for using these samples from MCMC procedures to produce approximated probabilistic forests.

The first is to use the mixture-of-parameters method.
Often, the predictive distribution, conditional on \( \btheta \), is available in closed form.
Call this \( F_c \left( x \mid \btheta \right) \).
We can approximate \( F \) by 
\begin{equation}
    \hat{F}^\mathrm{MP}(x) = \frac{1}{n} \sum_{i=1}^n F_c \left( x \mid \btheta_i \right).
\end{equation}
Alternatively, we can work directly with samples from the posterior predictive, \( \left( x^*_1, \ldots, x^*_n \right) \).
We could estimate \( F \) by the empirical cumulative distribution function of these samples:
\begin{equation}
    \hat{F}^\mathrm{ECDF}(x) = \frac{1}{n} \sum_{i=1}^n \1{x \geq x^*_i}.
\end{equation}
We could also estimate the predictive density, \( f \) by a kernel density estimate of the posterior predictive samples.

The authors recommend using the mixture-of-parameters method when possible, as it tends to produce scores closer to those obtained using the true posterior predictive distribution.
When the mixture-of-parameters method cannot be used, the authors advocate for the use of the ECDF method over the kernel density method.

Forecasts may be represented with less detail than a full probabilistic forecast distribution (or samples from such a distribution).
If, instead, only quantiles of the forecast distribution are provided, it is common to use the pinball loss function  to assess forecast quality:
\begin{equation}
    \operatorname{Pinball-Loss}(f_\alpha, x) = \left( \mathds{1} \left\{ x \leq f_\alpha \right\} - \alpha \right) \left( f_\alpha - x \right) =
    \begin{cases}
    \left( 1 - \alpha \right) \left( f_\alpha - x \right)   &   x \leq f_\alpha\\
    \alpha \left( x - f_\alpha \right)  &   x \geq f_\alpha
    \end{cases},
    \label{ch_2:eqn:pinball_loss}
\end{equation}
where \( x \) is the observed value and \( f_\alpha \) is the \( \alpha \) quantile of the predictive distribution.
When \( \alpha = 0.5 \), this is the same as \( 1 / 2  \) of the absolute error.
When \( \alpha \neq 0.5 \), the penalty is weighted based on the probability of \( f_\alpha - x \) being positive or negative.

When forecasts are presented in an interval format (or intervals are constructed from quantiles), the Winkler score is commonly used:
\begin{equation}
    \operatorname{Winkler}_\alpha \left( l_\alpha, u_\alpha, x \right) = \left( u_\alpha - l_\alpha \right) + \frac{2}{\alpha} \left( l_\alpha - x \right) \mathds{1} \left\{ x < l_\alpha \right\} + \frac{2}{\alpha} \left( x - u_\alpha \right) \mathds{1} \left\{ x > u_\alpha \right\},
\end{equation}
where \( \left[ l_\alpha, u_\alpha \right] \) is a \( 100 (1 - \alpha) \)\% prediction interval.
In \citet{Bracher2021Evaluating}, a weighted interval score is developed for combining Winkler scores at several levels of \( \alpha \).
A recent overview of methods for evaluating forecast quantiles is available in \citet{Gneiting2023Model}.
Further discussion of all of the above scoring methods is presented in \citet{gneiting2007strictly}.

When only point forecasts are available, many standard loss functions are applicable to measure the error between an observed value, \( x \), and a forecasted value, \( \hat{x} \).
The most common of these are the absolute error, \( \abs{x - \hat{x}} \), and the squared error, \( \left( x - \hat{x} \right)^2 \).
These measures are scale-dependent, making them inappropriate for comparing forecasts on quantities of different units.
To overcome this limitation, the absolute percentage error \( \abs{100 \left( x - \hat{x} \right)/ x} \) can be used.
A detailed discussion of error measurements available for point forecasts is presented in \citet{Hyndman2006Another}.
