\chapter{Background}
\label{ch:background}
% Aiming to hit around page 30 by the end
\section{Bayesian Inference}
\label{ch_2:sec:bayesian-mcmc}

When performing Bayesian statistical inference, we aim to quantify uncertainty about parameters of interest, \( \btheta \), conditional on some observed data, \( \bX \).
To accomplish this, we employ a prior distribution, which expresses our beliefs about \( \btheta \) without data, and a sampling distribution for \( \bX \).
Formally, by Bayes' theorem and the law of total probability, we have 

\begin{equation}
    \pi \left( \btheta \mid \bX \right) = \frac{\pi \left( \bX \mid \btheta \right) \pi \left( \btheta \right)}{\pi \left(  \bX \right)} = \frac{\pi \left( \bX \mid \btheta \right) \pi \left( \btheta \right)}{\int \pi \left( \bX \mid \btheta \right) \pi \left( \btheta \right) \rmd \btheta},
\end{equation}

where \( \pi \left( \bX \mid \btheta \right) \) is the likelihood for \( \bX \) and \( \pi \left( \btheta \right) \) is the prior distribution.
Because \( \pi \left( \bX \right) \) is constant, with respect to \( \btheta \), we typically work with an unnormalized posterior

\begin{equation}
    \pi \left( \btheta \mid \bX \right) \propto \pi \left( \bX \mid \btheta \right) \pi \left( \btheta \right).
\end{equation}

In practice, the posterior distribution, \( \pi \left( \btheta \mid \bX \right) \), rarely exists in closed form.
However, we can approximate the posterior distribution via Markov chain Monte Carlo, wherein we construct a Markov Chain whose stationary distribution is the posterior distribution.

A foundational algorithm for sampling from the posterior distribution is the Metropolis-Hastings algorithm \citep{Hastings1970Monte}.
In this algorithm, at each step in the Markov chain, a new state, \( \btheta^\prime \), is proposed by sampling from a proposal distribution \( q \).
The proposed value \( \btheta^\prime \), is accepted with probability 
\begin{align}
\alpha \left( \btheta, \btheta^\prime \right)   =&   \min \left\{ 1, \frac{\pi \left( \btheta^\prime \mid \bX \right)}{\pi \left( \btheta \mid \bX \right)} \frac{q \left( \btheta \mid \btheta^\prime \right)}{q \left( \btheta^\prime \mid \btheta  \right)} \right\}    \\
=&  \min \left\{ 1, \frac{\pi \left( \bX \mid \btheta^\prime  \right)}{\pi \left( \bX \mid \btheta \right)} \frac{\pi \left( \btheta^\prime \right)}{\pi \left( \btheta \right)} \frac{q \left( \btheta \mid \btheta^\prime \right)}{q \left( \btheta^\prime \mid \btheta  \right)} \right\}.
\end{align}

Efficiently exploring the posterior distribution using Metropolis-Hastings requires selecting a proposal distribution that produces proposals that are far enough away from the current parameters to explore the parameter space quickly, but not so widely distributed that they have a very low probability of acceptance.
As \( \btheta \) increases in dimensionality, striking this balance becomes more difficult, and Markov chains constructed with relatively simple algorithms tend to become ``stuck" in a small region, unable to explore the full posterior.
Sophisticated methods, such as the Metropolis-adjusted Langevin algorithm \citep{Besag1994Comments}, elliptical slice sampling \citep{Murray202Elliptical}, and Hamiltonian Monte Carlo \citep{Duane1987Hybrid} have been developed to improve this efficiency in high dimensional settings.

Throughout this dissertation, we rely on the Hamiltonian Monte Carlo.
Briefly, Hamiltonian Monte Carlo is a form of the Metropolisâ€“Hastings algorithm which uses Hamiltonian dynamics to generate proposals.
To accomplish this, the parameter space is augmented with ``momentum" variables which guide the proposals through regions of high probability.
For a detailed overview, see \citep{neal2011mcmc} and \citep{betancourt2018conceptual}.

\section{Confidence Distributions}
\label{ch_2:sec:confidence_distributions}
Confidence distributions are a frequentist estimators of a parameter, although they are historically associated with the fiducial distribution from fiducial statistics \citep{Xie2013}.
They are ``distribution estimators," similar in spirit to the bootstrap and the posterior distribution from Bayesian inference (see section \ref{ch_2:sec:bayesian-mcmc}).
A modern definition and short history of confidence distributions is available in \citep{Xie2013}.
For clarity, we present a more classical derivation, which relies on previously derived confidence interval procedures.

Under the classical derivation, we can create random variables associated with the lower and upper confidence distributions from two one-sided, nested, confidence interval procedures.
For a nested confidence interval procedure, the $(1-\alpha_1)$ interval is a subset of the $(1-\alpha_2)$ interval whenever $(1-\alpha_1) \leq (1-\alpha_2)$.
Denote those one-sided intervals by $[L({\bf x},1-\alpha), \infty)$ and $(-\infty, U({\bf x},1-\alpha)]$, which are defined for any observed data ${\bf x}$, and $(1-\alpha) \in (0,1)$.
For fixed ${\bf x}$, define the lower and upper confidence distribution random variables as $L({\bf x}, A)$ and $U({\bf x},B)$, where $A$ and $B$ are independent uniform random variables.
Then a $100(1-\alpha)\%$ central confidence interval has the $(\alpha/2)$th quantile of $L({\bf x}, A)$ as the lower limit, and the $(1-\alpha/2)$th quantile of $U({\bf x},B)$ as the upper limit.

As an example, let \( X_1, \ldots, X_n \) be independently and identically distributed \( \textrm{Normal}(\mu, \sigma^2) \).
With \( \sigma^2 \) known, a \( (1 - \alpha) \)\% confidence interval for \( \mu \) is
\begin{equation}
\left[ \bar{x} - z_{\alpha/2} \frac{\sigma}{\sqrt{n}}, \bar{x} + z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \right],
\label{ch_2:eqn:CI_normal_1}
\end{equation}
where \( \bar{x} \) is the sample mean and \( z_{\alpha / 2} \) satisfies \( P(Z \geq z_{\alpha / 2}) = \alpha / 2 \) and \( Z \sim \textrm{Normal}(0,1) \).

Letting \( L_\mu (\bx, q) = U_\mu (\bx, q) = \bar{x} - \frac{\sigma}{\sqrt{n}} \Phi^{-1}(q) \), we have \( L_\mu(\bx, A) \sim \textrm{Normal} \left( \bar{x}, \frac{\sigma^2}{n} \right) \) and \( U_\mu(\bx, A) \sim \textrm{Normal} \left( \bar{x}, \frac{\sigma^2}{n} \right) \).

We now note that \eqref{ch_2:eqn:CI_normal_1} is equivalent to
\begin{equation}
    \left[ F_{L_{\mu}}^{-1}(\alpha / 2), F_{U_{\mu}}^{-1}(1- \alpha / 2) \right],
\end{equation}
where \( F_Y^{-1} \) indicates the inverse of the cumulative distribution function of a random variable \( Y \).
Thus, by definition, \( L_\mu \) and \( U_\mu \) are lower and upper confidence distribution random variables for \( \mu \).

While the focus of our work is on combining confidence distributions of different variables to produce confidence intervals, the distributions have additional uses.
For instance, we can create point estimators by considering the mean, median, or mode of a confidence distribution.
Under modest conditions, these can be shown to be consistent estimators.
We can also perform hypothesis tests using confidence distributions.
For a one-sided hypotheses test at the level \( \alpha \) of \( H_0: \theta \in C \) versus \( H_0: \theta \notin C \), where \( C \) is an interval of the form \( [b, \infty] \) or \( -\infty, b \), we reject \( H_0 \) if the integral of the confidence distribution under \( C \) is less than \( \alpha \).

\section{Mathematical Models for the Spread of Infectious Diseases}
\label{sec:math_models}

\subsection{Compartmental Models}
\label{ch_2:sec:compartmental_models}
The compartmental model is a popular tool used to describe the spread of an infectious disease agent.
The most basic and prominent of these models is the Susceptible-Infected-Removed (SIR) model, which we use as an example in this section.
In these models, the population of interest is divided into compartments that indicate a disease status (e.g., susceptible, infectious, and recovered), and individuals transition between compartments at a specified rate
(e.g., transitions from infectious compartment to recovered compartment happen at a rate proportional to the number of infectious individuals).
Overviews of mechanistic compartmental models for disease dynamics can be found in \citep{anderson1992infectious, Brauer2008, keeling2011modeling, 10.1093/aje/kww021}.

Compartmental models can be represented deterministically or stochastically.
In the stochastic treatment, the model is formulated as a continuous-time Markov chain (CTMC).
A CTMC is a stochastic process \( \left\{ X_t \right\}, t \geq 0 \), on a state space \( \Omega \) such that
\begin{equation}
    \Pr\left( X_{t + s} = j\mid X_s = i, X_{s_n} = i_n, \ldots, X_{s_0} = i_0 \right) = \Pr \left( X_{t+s} = j \mid X_s = i\right),
\end{equation}
for any \(0 <  s_0 < \cdots < s_n < s \) and \( i_0, \ldots, i_n, i, j \in \Omega \).
For the SIR model with closed population size \( N \), the state space is \( \left\{ (S, I, R) \in \left[ 0, N \right]^3 \cap \mathbb{N}^3 \mid S + I + R = N\right\} \), where \( S \), \( I \), and \( R \) indicate the number of people from the population in the susceptible, infectious, and recovered compartments, respectively.
Because \( N \) is fixed, the state space can be fully defined using any two of \( S \), \( I \), and \( R \), with the third implicitly defined as \( N \) minus the sum of the other two.
We use the triplet representation for clarity.

In this model, individuals may only transition from susceptible to infectious \( (S \to I) \) or from infectious to recovered \( (I \to R) \) (see Figure~\ref{ch_2:fig:SIR_diagram}).
Infection events are assumed to happen at the rate \( \frac{\beta}{N} S I \), where \( \beta \) is the contact rate.
Recovery events occur at a rate \( \gamma I \), where \( \gamma \) is the recovery rate.
The diagram in Figure~\ref{ch_2:fig:stocastic_SIR} represents the system's evolution from one state to the next.

\begin{figure}
    \centering
% https://q.uiver.app/?q=WzAsMyxbMCwwLCJTIl0sWzEsMCwiSSJdLFsyLDAsIlIiXSxbMCwxXSxbMSwyXV0=
\begin{tikzcd}
	S & I & R
	\arrow[from=1-1, to=1-2]
	\arrow[from=1-2, to=1-3]
\end{tikzcd}
    \caption[The basic SIR compartmental model.]{The basic SIR compartmental model.
    Only transitions from susceptible to infectious \( (S \to I) \) and from infectious to recovered \( (I \to R) \) are allowed.}
    \label{ch_2:fig:SIR_diagram}
\end{figure}

\begin{figure}
    \centering
% https://q.uiver.app/?q=WzAsMyxbMCwxLCIoUyh0KSwgSSh0KSwgUih0KSkiXSxbMiwwLCIoUyh0KS0xLCBJKHQpKzEsIFIodCkpIl0sWzIsMiwiKFModCksIEkodCktMSwgUih0KSsxKSJdLFswLDEsIlxcZnJhY3tcXGJldGF9e059U0kiXSxbMCwyLCJcXGdhbW1hIEkiLDJdLFsxLDAsIlxcdGV4dHtJbmZlY3Rpb259IiwwLHsic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoibm9uZSJ9LCJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzIsMCwiXFx0ZXh0e1JlY292ZXJ5fSIsMix7InN0eWxlIjp7ImJvZHkiOnsibmFtZSI6Im5vbmUifSwiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dXQ==
\begin{tikzcd}
	&& {(S-1, I+1, R)} \\
	{(S, I, R)} \\
	&& {(S, I-1, R+1)}
	\arrow["{\frac{\beta}{N}SI}", from=2-1, to=1-3]
	\arrow["{\gamma I}"', from=2-1, to=3-3]
	\arrow["{\text{Infection}}", draw=none, from=1-3, to=2-1]
	\arrow["{\text{Recovery}}"', draw=none, from=3-3, to=2-1]
\end{tikzcd}
    \caption[State transition graph for SIR model.]{State transition graph for SIR model with current state \( (S \), \( I \),  \( R) \).
    The population experiences an infection and transitions to the state \( (S-1, I+1, R) \) at the rate \( \frac{\beta}{N}SI \).
    The population experiences a recovery and transitions to the state \( (S, I-1, R+1) \) at the rate \( \gamma I \).
    }
    \label{ch_2:fig:stocastic_SIR}
\end{figure}

The deterministic treatment of a stochastic compartmental model is a system of ordinary differential equations (ODEs).
For the SIR model, we have 
\begin{equation}
    \deriv{S}{t} = -\beta \frac{SI}{N}, \qquad
    \deriv{I}{t} = \beta \frac{SI}{N} - \gamma I, \qquad
    \text{and} \qquad
    \deriv{R}{t} = \gamma I
    \text{.}
\label{ch_2:SIR_diff_eq}
\end{equation}
Letting \( s(t) = S(t) / N \), \( i(t) = I(t) / N \), \( r(t) = R(t) / N \), where \( A(t) \) is the value of compartment \( A \) at time \( t \), the system of ODEs can be viewed as the diffusion limit of the stochastic model as \( N \to \infty \), while \( s(0), i(0), r(0) \) are held constant \citep{Greenwood2009, fuchs2013inference}.
% The latter corresponds to a deterministic model and can be obtained by ignoring the stochastic part of
% which is the infinite population limit of the stochastic model.


Compared to the deterministic models, the stochastic models are more computationally difficult to work with, but they are particularly useful when there are few infections, as at the beginning or end of an outbreak and in small populations.
In these scenarios, the randomness involved at the subject level can have a major impact on the course of the outbreak.
In an extreme case, one infectious individual could be introduced to a population, but, by chance, not infect anyone else in the population.
Then, no outbreak occurs.
In the deterministic treatment, the outbreak is guaranteed to begin and will last indefinitely because \( S(t) \), \(  I(t) \), and \( R(t) \) will always be greater than 0, due to the behavior of the differential equations in \eqref{ch_2:SIR_diff_eq}.
The benefit to using the less realistic deterministic models is that they are more computationally feasible and tend to work well when all compartments are suitably large \citep{doi:10.1098/rspb.2015.0347}.
This work focuses on the deterministic models, as we typically work in large population settings, where an outbreak is ongoing.

The basic SIR model can be augmented with more compartments, as well as time-varying parameters, to more realistically represent an outbreak.
Additional compartments can account for different disease states (e.g., vaccinated or deceased), competing disease variants, or stratification of the population (e.g., by age or location).
It is also possible to model an endemic disease by allowing reinfection after recovery (e.g., \(R \to S\) transitions).
Time-varying parameters can also model changes in behavior or policy (e.g., stay at home orders and restaurant closures) that cannot be captured by the constant parameters.

\begin{figure}
    \centering
% https://q.uiver.app/?q=WzAsNSxbMCwxLCJTIl0sWzQsMSwiSSJdLFs1LDAsIlIiXSxbMiwxLCJFIl0sWzUsMiwiRCJdLFsxLDJdLFswLDNdLFszLDFdLFsxLDRdLFsyLDAsIiIsMSx7ImN1cnZlIjo1fV1d
\begin{tikzcd}[column sep=scriptsize]
	&&&&& R \\
	S && E && I \\
	&&&&& D
	\arrow[from=2-5, to=1-6]
	\arrow[from=2-1, to=2-3]
	\arrow[from=2-3, to=2-5]
	\arrow[from=2-5, to=3-6]
	\arrow[curve={height=30pt}, from=1-6, to=2-1]
\end{tikzcd}
    \caption[Augmented SIR model.]{
    Augmented SIR model.
    The states in this model are susceptible \( (S) \), exposed \( (E) \), infectious \( (I) \), Recovered \( (R) \) and Deceased \( D \).}
    \label{ch_2:fig:SEIRDS_diagram}
\end{figure}

Figure~\ref{ch_2:fig:SEIRDS_diagram} presents a model demonstrating some of these features, including additional compartments for individuals who are infected, but not infections (\( E \)) and those who are deceased (\( D \)), as well as loss of immunity and possibility of reinfections after recovery.

\subsection{Data Integration}
\label{ch_2:sec:data_integration}
We are interested in estimating parameters of SIR-like models using surveillance data.
Broadly, we can categorize data used for this modeling as either actively or passively collected surveillance data.
Common examples of passively collected incidence data include daily counts of new diagnostic tests, confirmed cases, and deaths, as well as genetic sequences of viruses or bacteria that cause diseases.
We refer to this as incidence data because each count is assumed to correspond to a new event during that time.
In contrast, we can work with prevalence data, which is reflective of the frequency of a condition at a given time.
Examples of prevalence data include counts of current hospital or intensive care unit (ICU) occupants with a certain disease or the number of seropositive tests from a carefully constructed population sample.
We also loosely differentiate between data that is passively or actively collected.
Passively collected data like case counts and hospital occupancy is routinely reported to health care agencies and is generally more affordable to collect because it does not require a formal study to be conducted.
Actively collected data is often the result of a survey that has been deliberately designed and conducted to measure some aspect of a population, like the number of people previously infected with a disease or the contact rates between different subpopulations.
This data is more expensive to collect, but can provide information that is difficult to gather through passive surveillance.
% could mention mobility data somewhere here

As alluded to in Section~\ref{ch_2:sec:compartmental_models}, modelers can construct compartmental models which encode arbitrarily complex dynamics.
While simulating from these models can be straightforward, connecting them to real-world data to perform statistical inference is a non-trivial task.
Broadly, a model uses a compartmental model and basic parameters, like the reproductive number, duration of infectiousness, and infection-fatality ratio, to model the size of a compartment at a series of time points.
These compartment sizes can be mapped to prevalence data with some emission distribution or loss function.
To integrate incidence data, we can keep track of the cumulative incidence (e.g., by counting the cumulative number of \( S \to I \) transitions) at each time point and compute the difference between successive times.
Then these differences can be mapped to incidence data with an emission distribution or loss function.
For example, let \( N_{S I}(t_l) \) be the cumulative number of \( S \to I \) transitions by time \( t = l \), as tracked by the compartmental model.
Then \( \Delta N_{S I}(t_l) = N_{S I}(t_l) - N_{S I}(t_{l-1}) \) is the number new \( S \to I \) transitions between time \( t = l \) and time \( t = l - 1 \).
We can link this to \( C_l \), the number of observed cases between time \( t = l \) and time \( t = l - 1 \) by the following distributional assumption:
\begin{equation}
C_l \sim \text{Negative binomial}\left (\mu_l =  \rho \times \Delta N_{S I}(t_l),\ {\sigma^2_l} = \mu_l (1 + \mu_l / \phi )\right ),
\label{ch_2:eqn:example_case_emission}
\end{equation}
where \( \mu_l \) and \( \sigma^2_l \) are the mean and variance of this negative binomial distribution, \( \rho \) is a case reporting probability, reflecting that not all people who become infected with a disease are tested, and \( \phi \) is an over-dispersion parameter.

The specifics of the data and compartmental model used can make the mapping from the latent compartments to the observed data challenging.
In some cases, the distinction between incidence and prevalence data is not clear.
For example, when someone receives a positive test result for an infectious disease with a relatively short period of infectiousness, they may receive several tests in the following weeks to assess whether they are still infectious.
In that case, each positive test is not necessarily reflective of a new disease case.
The link between positive test results and the underlying cases may also be complicated by imperfect specificity and sensitivity of the tests.
Similarly, there is ambiguity about the disease's relevance to an event reported in the data.
Someone admitted to the hospital with a broken leg may also have an infectious disease and be reported as part of the hospital's passive surveillance data.
Thus, the hospitalization data may not be truly reflective of the number of people with severe disease cases.
Often, there is a significant delay between when data is captured and when it is reported.
When performing inference or predictions in real time, it may appear that there are fewer cases in one week than in the previous week, simply due to the most recent week's tests not yet being fully reported.
Furthermore, reporting practices for passive surveillance data may be inconsistent.
The inconsistency could be due to any number of reasons.
For example, many testing locations may not be open on the weekends or holidays, making it appear that new cases are less common on those days.
Inconsistencies may also be due to policy changes, changes in funding, shifting demographics among the infected population, or even accidental non-reporting.

While overcoming each of the above individual complications is itself difficult and requires modelers to make many subtle choices, there is additional complexity involved in integrating each of the data streams in a unified model.
This additional complexity can be worth pursuing, as integrating more data sources may make more model parameters identifiable.
\citet{DeAngelis2015four} present four important challenges in data integration when modeling infectious diseases.
The first is the weighting of evidence.
When combining data streams of varying quality, it is desirable to rely more on the high-quality data than the low-quality data.
This can be done simply by excluding the low-quality data altogether, or by formally modeling data quality issues.
Under the umbrella of model criticism, the authors present identifiability, conflict, and influence as areas of concern.
In Section~\ref{ch_1:sec:motivating_examples}, we provided an example of different data sources presenting conflicting evidence about underlying disease dynamics.
Understanding how each data stream informs modeling results and how their apparent differences are reconciled is a key challenge.
In addition, the authors cite issues related to computational complexity when combining many data sources, as well as potential problems of dependencies between the datasets included in the model.

\section{Forecasting Infectious Disease Outbreaks}
\label{sec:forecasting_techniques_and_assessment}

While compartmental models are a common framework for forecasting infectious disease outbreaks, other methods are available for the task.
A sample of these methods are gathered in the COVID-19 Forecast Hub, which collected and synthesized forecasts from a variety of modelers with diverse approaches throughout the COVID-19 pandemic \citep{Cramer2022Evaluation}.
Among the methods presented are those using compartmental models, along with approaches common to other areas of statistics, such as time series and machine learning methods.
In addition, \citet{Cramer2022Evaluation} evaluate an ensemble forecast, which combines the models into a single forecast, which is generally shown to have superior performance to any of the individual models.
These methods typically use the data described in Section~\ref{ch_2:sec:data_integration}, but some incorporate additional data sources, such as mobility data collected from mobile phones.

\subsection{Forecast Assessment}

Methods used for forecast assessment differ based on the format of the forecasts.
We begin with detailed exploration of forecasts in the form of full probability distributions, which we use throughout this work.
We end this section with a brief overview of other forecast evaluation methods.

Probabilistic forecasts are probability distributions over future events.
In this section, let \( F(x) \) denote the cumulative distribution function of a forecast.
We assume that new data are realized samples from some underlying probability distribution \( G \), which is only known to nature.
Thus, an ideal forecast is \( F = G \).
Because \( G \) is unknown, assessing whether \(F = G\) is impossible.
The sharpness principle conjectures that issuing a forecast that maximizes sharpness, subject to calibration, is equivalent to making an ideal forecast \citep{Gneiting2007Probabilistic}.

In plain terms, a forecast is well-calibrated if events that are forecasted to have a \( y \)\% chance of happening, are actually observed \( y \)\% of the time.
Being well-calibrated is insufficient for a forecast to be ``good."
For example, consider the ``climatological" forecaster, who issues forecasts based on the marginal distribution of the predictand.
In the context of infectious disease modeling, a climatological forecaster could issue a forecast for daily influenza cases by issuing the same forecast every day, where their forecast is simply a smoothed estimate of daily influenza cases from the last decade.
Assuming that influenza case counts in the future are, on average, distributed as they were in the last decade, this forecaster will be well-calibrated.
However, competing forecasters should be able to make narrower forecasts while maintaining calibration, perhaps by using data from the preceding week to predict case counts the next week.
These forecasts are clearly more useful and are said to be ``sharper."

In practice, calibration and sharpness can be assessed by both numerical and graphical summaries.
Calibration is often assessed by computing the probability integral transform (PIT), \( p_t = F_t(x_t) \) for each forecast, \( F_t \), and its corresponding observed value, \( x_t \), for all forecast times \( t \).
If the forecasts are well-calibrated, \( p_t \sim \operatorname{Uniform}(0,1) \) and the \( p_t \)'s will exhibit no autocorrelation.
Visual inspection of a histogram of PIT values for uniformity can, therefore, diagnose issues of miscalibration.
Calibration can also be assessed by computing the empirical coverage of prediction intervals.
Sharpness can be assessed by visually inspecting the predictive distributions or by computing summaries like interval widths or variance of the distribution.

To numerically assess both sharpness and calibration, a strictly proper scoring rule can also be used.
We denote a score for a probabilistic forecast \( F \) and observed outcome \( x \) by \( s(F, x) \).
Generally, the forecaster wishes to minimize the score, which can be considered a penalty or cost function.
A scoring rule is said to be proper when the expected value of the score is minimized when \( F = G \), where $x \sim G$.
The scoring rule is strictly proper when this minimum is unique.

For continuous variables, the continuous ranked probability score is a popular scoring rule with attractive properties:
\begin{equation}
    \operatorname{CRPS}(F, x) = \int_{-\infty}^{\infty} \left( F(y) - \mathds{1} \left( y \geq x \right) \right)^2 \rmd y.
    \label{ch_2:eqn:crps}
\end{equation}
In contrast to other scoring rules, it is based on the cumulative distribution function of the predictive distribution, rather than the density function, which may not exist in some contexts.
Additionally, the CRPS is sensitive to distance, meaning that it gives credit to forecasts which assign high probability to values near \( x \), even if they do not assign high probability to \( x \) itself.
For a discussion of scoring rules, see \citep{gneiting2007strictly}.

Predictive distributions arise naturally from the posterior predictive distribution in the Bayesian setting.
The posterior predictive distribution is the distribution of possible unobserved values, \( \bX^* \), conditional on the observed values, \( \bX \).
Following the notation from Section~\ref{ch_2:sec:bayesian-mcmc}, the posterior predictive distribution is \( \pi \left( \bX^* \mid \bX \right) = \int \pi \left( \bX^* \mid \btheta \right) \pi \left( \btheta \mid \bX \right) \rmd \btheta \).
When working in the context of MCMC (Section~\ref{ch_2:sec:bayesian-mcmc}), the posterior predictive distribution, \( F \), is not available in closed form.
Rather, we only have some finite number of samples from the posterior distribution, \( \left( \btheta_i, \ldots \btheta_n \right) \).
\citet{kruger2021predictive} present three options for using these samples from MCMC procedures to produce approximated probabilistic forests.

The first is to use the mixture-of-parameters method.
Often, the predictive distribution, conditional on \( \btheta \), is available in closed form.
Call this \( F_c \left( x \mid \btheta \right) \).
We can approximate \( F \) by 
\begin{equation}
    \hat{F}^\mathrm{MP}(x) = \frac{1}{n} \sum_{i=1}^n F_c \left( x \mid \btheta_i \right).
\end{equation}
Alternatively, we can work directly with samples from the posterior predictive, \( \left( x^*_1, \ldots, x^*_n \right) \).
We could estimate \( F \) by the empirical cumulative distribution function of these samples:
\begin{equation}
    \hat{F}^\mathrm{ECDF}(x) = \frac{1}{n} \sum_{i=1}^n \1{x^*_i \leq x}.
\end{equation}
We could also estimate the predictive density, \( f \) by a kernel density estimate of the posterior predictive samples.

The authors recommend using the mixture-of-parameters method when possible, as it tends to produce scores closer to those obtained using the true posterior predictive distribution.
When the mixture-of-parameters method cannot be used, the authors advocate for the use of the ECDF method over the kernel density method.

Forecasts may be represented with less detail than a full probabilistic forecast distribution (or samples from such a distribution).
If, instead, only quantiles of the forecast distribution are provided, it is common to use the pinball loss function  to assess forecast quality:
\begin{equation}
    \operatorname{Pinball-Loss}(f_\alpha, x) = \left( \mathds{1} \left\{ x \leq f_\alpha \right\} - \alpha \right) \left( f_\alpha - x \right) =
    \begin{cases}
    \left( 1 - \alpha \right) \left( f_\alpha - x \right)   &   x \leq f_\alpha,\\
    \alpha \left( x - f_\alpha \right)  &   x \geq f_\alpha,
    \end{cases},
    \label{ch_2:eqn:pinball_loss}
\end{equation}
where \( x \) is the observed value and \( f_\alpha \) is the \( \alpha \) quantile of the predictive distribution.
When \( \alpha = 0.5 \), this is the same as \( 1 / 2  \) of the absolute error.
When \( \alpha \neq 0.5 \), the penalty is weighted based on the probability of \( f_\alpha - x \) being positive or negative.

When forecasts are presented in an interval format (or intervals are constructed from quantiles), the Winkler score is commonly used:
\begin{equation}
    \operatorname{Winkler}_\alpha \left( l_\alpha, u_\alpha, x \right) = \left( u_\alpha - l_\alpha \right) + \frac{2}{\alpha} \left( l_\alpha - x \right) \mathds{1} \left\{ x < l_\alpha \right\} + \frac{2}{\alpha} \left( x - u_\alpha \right) \mathds{1} \left\{ x > u_\alpha \right\},
\end{equation}
where \( \left[ l_\alpha, u_\alpha \right] \) is a \( 100 (1 - \alpha) \)\% prediction interval.
In \citep{Bracher2021Evaluating}, a weighted interval score is developed for combining Winkler scores at several levels of \( \alpha \).
A recent overview of methods for evaluating forecast quantiles is available in \citep{Gneiting2023Model}.
Further discussion of all of the above scoring methods is presented by \citet{gneiting2007strictly}.

When only point forecasts are available, many standard loss functions are applicable to measure the error between an observed value, \( x \), and a forecasted value, \( \hat{x} \).
The most common of these are the absolute error, \( \abs{x - \hat{x}} \), and the squared error, \( \left( x - \hat{x} \right)^2 \).
These measures are scale-dependent, making them inappropriate for comparing forecasts on quantities of different units.
To overcome this limitation, the absolute percentage error \( \abs{100 \left( x - \hat{x} \right)/ x} \) can be used.
A detailed discussion of error measurements available for point forecasts is presented in \citep{Hyndman2006Another}.

When assessing any of the measures of forecast quality mentioned in this section, it is useful to aggregate them at a fixed forecast horizon.
For example, for a $j$-week forecast horizon, we create a forecast for week \( i \) using all data up to week \( i - j \).
After repeating this for all weeks of interest, we can compute the relevant score or loss for each forecast.
We can then compare multiple forecasting procedures by comparing the average score or loss for each model at the given forecast horizon.