\chapter{Background}
\label{ch:background}
% 8-10 pages
\section{Bayesian Inference}
\label{ch_2:sec:bayesian-mcmc}

When performing Bayesian statistical inference, we aim to quantify uncertainty about parameters of interest, \( \btheta \), conditional on some observed data, \( \bY \).
To accomplish this, we employ a prior distribution, which expresses our beliefs about \( \btheta \) without data, and a sampling distribution for \( \bY \).
Formally, by Bayes' theorem and the law of total probability, we have 

\begin{equation}
    \pi \left( \btheta \mid \bY \right) = \frac{\pi \left( \bY \mid \btheta \right) \pi \left( \btheta \right)}{\pi \left(  \bY \right)} = \frac{\pi \left( \bY \mid \btheta \right) \pi \left( \btheta \right)}{\int \pi \left( \bY \mid \btheta \right) \pi \left( \btheta \right) \rmd \btheta},
\end{equation}

where \( \pi \left( \bY \mid \btheta \right) \) is the sampling distribution for \( \bY \) and \( \pi \left( \btheta \right) \) is the prior distribution.
Because \( \pi \left( \bY \right) \) is constant, with respect to \( \btheta \), we typically work with the unnormalized posterior

\begin{equation}
    \pi \left( \btheta \mid \bY \right) \propto \pi \left( \bY \mid \btheta \right) \pi \left( \btheta \right).
\end{equation}

In practice, the posterior distribution, \( \pi \left( \btheta \mid \bY \right) \), rarely exists in closed form.
However, we can sample from the posterior distribution via Markov chain Monte Carlo, wherein we construct a Markov Chain whose stationary distribution is the posterior distribution.
In this work, we rely on Hamiltonian Monte Carlo.

Briefly, Hamiltonian Monte Carlo is a form of the Metropolis–Hastings algorithm.
In the Metropolis-Hastings algorithm, at each step in the Markov chain, a new state, \( \btheta^\prime \), is proposed by sampling from a proposal distribution \( q \).
The proposed value \( \btheta^\prime \), is accepted with probability 

\begin{align}
\alpha \left( \btheta, \btheta^\prime \right)   =&   \min \left\{ 1, \frac{\pi \left( \btheta^\prime \mid \bY \right)}{\pi \left( \btheta \mid \bY \right)} \frac{q \left( \btheta \mid \btheta^\prime \right)}{q \left( \btheta^\prime \mid \btheta  \right)} \right\}    \\
=&  \min \left\{ 1, \frac{\pi \left( \bY \mid \btheta^\prime  \right)}{\pi \left( \bY \mid \btheta \right)} \frac{\pi \left( \btheta^\prime \right)}{\pi \left( \btheta \right)} \frac{q \left( \btheta \mid \btheta^\prime \right)}{q \left( \btheta^\prime \mid \btheta  \right)} \right\}.
\end{align}

Efficiently sampling using Metropolis-Hastings requires selecting a proposal distribution that produces proposals that are far enough away from the current parameters to explore the parameter space quickly, but not so far that they have a very low probability of acceptance.
As \( \btheta \) increases in dimensionality, striking this balance becomes more difficult and Markov chains tend to become ``stuck" in a small region, unable to explore the full posterior.
Compared to simpler algorithms (e.g. random walk Metropolis-Hastings), Hamiltonian Monte Carlo can more efficiently generate proposals by using Hamiltonian dynamics, where we augment our parameter space with ``momentum" variables which guide the proposals through regions of high probability.
For a detailed overview, see \citet{betancourt2018conceptual} and \citet{neal2011mcmc}.

\section{Confidence Distributions}
\label{ch_2:sec:confidence_distributions}
Confidence distributions are a frequentist estimator of a parameter, although they are historically associated with the fiducial distribution from fiducial statistics \citep{Xie2013}.
They are ``distribution estimators," similar in spirit to the bootstrap (????) and the posterior distribution from Bayesian inference (see section \ref{ch_2:sec:bayesian-mcmc}).
A modern definition and short history of confidence distributions is available in \citet{Xie2013}.
For clarity with respect to discrete data, which is the focus of our use of confidence distributions in Chapter~\ref{ch:content_1}, we present a more classical derivation, which relies on previously derived confidence interval procedures.

There are two confidence distributions (the lower and upper ones), used to ensure validity of the resulting inferences.
Under the classical derivation, we can create random variables associated with the lower and upper confidence distributions from two one-sided, nested, confidence interval procedures, where for a nested confidence interval procedure, the $(1-\alpha_1)$ interval is a subset of the $(1-\alpha_2)$ interval whenever $(1-\alpha_1) \leq (1-\alpha_2)$.
Denote those one-sided intervals by $[L({\bf x},1-\alpha), \infty)$ and $(-\infty, U({\bf x},1-\alpha)]$, which are defined for any observed data ${\bf x}$, and $(1-\alpha) \in (0,1)$.
For fixed ${\bf x}$, define the lower and upper confidence distribution random variables as $L({\bf x}, A)$ and $U({\bf x},B)$, where $A$ and $B$ are independent uniform random variables.
Then a $100(1-\alpha)\%$ central confidence interval has the $(\alpha/2)$th quantile of $L({\bf x}, A)$ as the lower limit, and the $(1-\alpha/2)$th quantile of $U({\bf x},B)$ as the upper limit.

As an example, let \( X_1, \ldots, X_n \) be independently and identically distributed \( \textrm{Normal}(\mu, \sigma^2) \).
With \( \sigma^2 \) known, a \( (1 - \alpha) \)\% confidence interval for \( \mu \) is:

\begin{equation}
\left[ \bar{x} - z_{\alpha/2} \frac{\sigma}{\sqrt{n}}, \bar{x} + z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \right],
\label{ch_2:eqn:CI_normal_1}
\end{equation}


where \( \bar{x} \) is the sample mean and \( z_{\alpha / 2} \) satisfies \( P(Z \geq z_{\alpha / 2}) = \alpha / 2 \) and \( Z \sim \textrm{Normal}(0,1) \).

Letting \( L_\mu (\bx, q) = U_\mu (\bx, q) = \bar{x} - \frac{\sigma}{\sqrt{n}} \Phi^{-1}(q) \), we have \( L_\mu(\bx, A) \sim \textrm{Normal} \left( \bar{x}, \frac{\sigma^2}{n} \right) \) and \( U_\mu(\bx, A) \sim \textrm{Normal} \left( \bar{x}, \frac{\sigma^2}{n} \right) \)

We now note that \eqref{ch_2:eqn:CI_normal_1} is equivalent to

\[ \left[ F_{L_{\mu}}^{-1}(\alpha / 2), F_{U_{\mu}}^{-1}(1- \alpha / 2) \right], \]

where \( F_Y^{-1} \) indicates the inverse of the cumulative distribution function of a random variable \( Y \).
Thus, we say \( L_\mu \) and \( U_\mu \) are the lower and upper confidence distribution random variables for \( \mu \).

\section{Mathematical Models for the Spread of Infectious Diseases}
\label{sec:math_models}

\subsection{Compartmental Models}

The compartmental model is a popular tool used to describe the spread of infectious disease.
The most basic and prominent of these is the Susceptible-Infected-Removed (SIR) model.
In these models, the population of interest is divided into compartments which indicate a disease status (e.g., susceptible, infectious, and recovered), and individuals transition between compartments at a specified rate
(e.g. transitions from infectious compartment to recovered compartment happen at a rate proportional to the number of infectious individuals).
Overviews of mechanistic compartmental models for disease dynamics can be found in \citet{anderson1992infectious, Brauer2008, keeling2011modeling, 10.1093/aje/kww021}.

These models can be represented deterministically or stochastically.
In the stochastic treatment, the model is formulated as a continuous-time Markov chain.
For the SIR model, we write the state vector at the time \( t \) as \( \bX(t) = (S(t), I(t), R(t)) \).
Where \( S(t), I(t) \), and \( R(t) \) indicate the number of people from the population in the susceptible, infectious, and recovered compartments, respectively.
In this model, individuals may only transition from susceptible to infectious \( (S \to I) \) or from infectious to recovered \( (I \to R) \) (see Figure~\ref{fig:ch_2:SIR_diagram}).
\begin{figure}
    \centering
% https://q.uiver.app/?q=WzAsMyxbMCwwLCJTIl0sWzEsMCwiSSJdLFsyLDAsIlIiXSxbMCwxXSxbMSwyXV0=
\begin{tikzcd}
	S & I & R
	\arrow[from=1-1, to=1-2]
	\arrow[from=1-2, to=1-3]
\end{tikzcd}
    \caption{The basic SIR compartmental model.
    Only transitions from susceptible to infectious \( (S \to I) \) and from infectious to recovered \( (I \to R) \) are allowed.}
    \label{fig:ch_2:SIR_diagram}
\end{figure}
Infection events are assumed to happen at the rate \( \frac{\beta}{N} S(t) I(t) \), where \( \beta \) is the contact rate
Recovery events occur at a rate \( \gamma I \), where \( \gamma \) is the recovery rate.
If we constrain the population to be closed, as is typically done, we have \(  S(t) + I(t) + R(t) = N \), where \( N \) is the total population size.
Thus, the state can be represented by any two elements of \( \bX (t) \), with the third implicitly defined as \( N \) minus the sum of the other two.
We use the full representation for clarity.
The diagram in Figure~\ref{ch_2:fig:stocastic_SIR} represents the system's evolution from one state to the next.

\begin{figure}
    \centering
% https://q.uiver.app/?q=WzAsMyxbMCwxLCIoUyh0KSwgSSh0KSwgUih0KSkiXSxbMiwwLCIoUyh0KS0xLCBJKHQpKzEsIFIodCkpIl0sWzIsMiwiKFModCksIEkodCktMSwgUih0KSsxKSJdLFswLDEsIlxcZnJhY3tcXGJldGF9e059U0kiXSxbMCwyLCJcXGdhbW1hIEkiLDJdLFsxLDAsIlxcdGV4dHtJbmZlY3Rpb259IiwwLHsic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoibm9uZSJ9LCJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzIsMCwiXFx0ZXh0e1JlY292ZXJ5fSIsMix7InN0eWxlIjp7ImJvZHkiOnsibmFtZSI6Im5vbmUifSwiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dXQ==
\begin{tikzcd}
	&& {(S(t)-1, I(t)+1, R(t))} \\
	{(S(t), I(t), R(t))} \\
	&& {(S(t), I(t)-1, R(t)+1)}
	\arrow["{\frac{\beta}{N}SI}", from=2-1, to=1-3]
	\arrow["{\gamma I}"', from=2-1, to=3-3]
	\arrow["{\text{Infection}}", draw=none, from=1-3, to=2-1]
	\arrow["{\text{Recovery}}"', draw=none, from=3-3, to=2-1]
\end{tikzcd}
    \caption{State transition diagram for SIR model at time \( t \), with current state \( (S(t), I(t), R(t)) \).
    The population experiences an infection and transitions to the state \( (S(t)-1, I(t)+1, R(t)) \) at the rate \( \frac{\beta}{N}SI \).
    The population experiences a recovery and transitions to the state \( (S(t), I(t)-11, R(t)+1) \) at the rate \( \gamma I \).
    }
    \label{ch_2:fig:stocastic_SIR}
\end{figure}

The deterministic treatment of a stochastic compartmental model is a system of ordinary differential equations, which is the infinite population limit of the stochastic model \citep{Greenwood2009}.
For the SIR model, we have 

\begin{equation}
    \deriv{S}{t} = -\beta \frac{SI}{N}, \qquad
    \deriv{I}{t} = \beta \frac{SI}{N} - \gamma I, \qquad
    \text{and} \qquad
    \deriv{R}{t} = \gamma I
    \text{.}
\label{ch_2:SIR_diff_eq}
\end{equation}

Compared to the deterministic models, the stochastic models are more computationally difficult to fit and are particularly useful when there are few infections, as at the beginning or end of an outbreak.
In these scenarios, the randomness involved at the subject level can have a major impact on the course of the outbreak.
In an extreme case, one infectious individual could be introduced to a population, but, by chance, not infect anyone else in the population.
Then, no outbreak occurs.
In contrast, deterministic models are more computationally feasible and tend to work well when all compartments are suitably large \citep{doi:10.1098/rspb.2015.0347}.
This work focuses on the deterministic models, as we typically work in large population settings, where an outbreak is ongoing.

Maybe this would be a good place to define \( R_0 \)?

The basic SIR model can be augmented with more compartments, as well as time-varying parameters, to more realistically represent an outbreak.
Additional compartments can account for different disease states (e.g. exposed, vaccinated, or deceased) or stratification of the population (e.g. by age or location).
It is also possible to model a perpetual outbreak by allowing reinfection after recovery (e.g. \(R \to S\) transitions).
Time-varying parameters can also model changes in behavior or policy (e.g. stay at home orders and restaurant closures) that cannot be captured by the constant parameters.

Figure~\ref{fig:ch_2:SEIRDS_diagram} presents a model demonstrating some of these features.

\begin{figure}
    \centering
% https://q.uiver.app/?q=WzAsNSxbMCwxLCJTIl0sWzQsMSwiSSJdLFs1LDAsIlIiXSxbMiwxLCJFIl0sWzUsMiwiRCJdLFsxLDJdLFswLDNdLFszLDFdLFsxLDRdLFsyLDAsIiIsMSx7ImN1cnZlIjo1fV1d
\begin{tikzcd}[column sep=scriptsize]
	&&&&& R \\
	S && E && I \\
	&&&&& D
	\arrow[from=2-5, to=1-6]
	\arrow[from=2-1, to=2-3]
	\arrow[from=2-3, to=2-5]
	\arrow[from=2-5, to=3-6]
	\arrow[curve={height=30pt}, from=1-6, to=2-1]
\end{tikzcd}
    \caption{Diagram of an augmented SIR model.
    The states in this model are susceptible \( (S) \), exposed \( (E) \), infectious \( (I) \), Recovered \( (R) \) and Deceased \( D \).}
    \label{fig:ch_2:SEIRDS_diagram}
\end{figure}

The model in Figure~\ref{fig:ch_2:SEIRDS_diagram} includes three new compartments, compared to the SIR model.

% \subsection{Logistic growth of variants}

\section{Probabilistic Forecast Assessment}
\label{sec:forecasting_techniques_and_assessment}

\begin{itemize}
\item https://epiforecasts.io/scoringutils/
\item https://www.jstatsoft.org/article/view/v090i12
\item Maybe cite some theory from Gneiting
\item Maybe cite some theory from Reich papers 
\item CRPS paper Krueger, F., Lerch, S., Thorarinsdottir, T.L. and T. Gneiting (2021): ‘Predictive inference based on Markov chain Monte Carlo output’, International Statistical Review 89, 274-301. doi:10.1111/insr.12405
\item scoringRules paper https://www.jstatsoft.org/article/view/v090i12
\item proper scoring rules

\end{itemize}

% 8-10 pages